{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5장\n",
    "\n",
    "## 5.1 계산 그래프\n",
    "\n",
    "### 5.1.1 계산 그래프로 풀다\n",
    "\n",
    "### 5.1.2 국소적 계산\n",
    "\n",
    "### 5.1.3 왜 계산 그래프로 푸는가?\n",
    "\n",
    "1. 전체가 아무리 복잡해도 각 노드에서는 단순 계산만 하면 된다\n",
    "2. 중간 계산 결과를 모두 보관 가능. \n",
    "(o. 장점일까? 이것으로 인한 암시적 메모리 기능이 있는데, neural turing machine 같은경우 그런 메모리를 분리하는 듯.)\n",
    "3. 진짜 이유. 역전파를 통해 '미분'을 효율적으로 계산하기 위해.\n",
    "\n",
    "(궁금. 미분을 계산하면 각 노드가 얼마나 최종값을 바꾸는지 알 수 있겠지만, 실제 쓰는건 손실함수인데 그게 정확히 어떻게 연계되는거지?)\n",
    "\n",
    "## 5.2 연쇄법칙\n",
    "\n",
    "### 5.2.2 연쇄법칙이란?\n",
    "\n",
    "> 연쇄법칙: 합성함수의 미분은 합성함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다.\n",
    "\n",
    "### 5.2.3 연쇄법칙과 계산 그래프\n",
    "\n",
    "## 5.3 역전파\n",
    "\n",
    "### 5.3.1 덧셈 노드의 역전파\n",
    "\n",
    "`z=x+y` 에서,\n",
    "`pdz/pdx=1` & `pdz/pdy=1`\n",
    "\n",
    "### 5.3.2 곱셈 노드의 역전파\n",
    "\n",
    "`z=xy`에서,\n",
    "`pdz/pdx=y` & `pdz/pdy=x`\n",
    "\n",
    "\n",
    "## 5.4 단순한 계층 구현하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x=None\n",
    "        self.y=None\n",
    "    \n",
    "    def foward(self,x,y):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "        \n",
    "    def backward(self,dout):\n",
    "        dx=dout*self.y\n",
    "        dy=dout*self.x\n",
    "        return dx,dy    \n",
    "    \n",
    "class AddLayer:\n",
    "    def __init_(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x,y):\n",
    "        out=x+y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, out):\n",
    "        dx=dout*1\n",
    "        dy=dout*1\n",
    "        return dx,dy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 활성화 함수 계층 구현하기\n",
    "\n",
    "### 5.5.1 ReLU 계층\n",
    "\n",
    "활성화 함수 ReLU의 수식:\n",
    "    \n",
    "\\begin{gather*}\n",
    "y=\\begin{cases}\n",
    "x & ( x >0)\\\\\n",
    "0 & ( x\\leqslant 0)\n",
    "\\end{cases}\\\\\n",
    "\\end{gather*}\n",
    "\n",
    "그의 미분:\n",
    "    \n",
    "\\begin{gather*}\n",
    "\\frac{\\partial y}{\\partial x} =\\begin{cases}\n",
    "1 & ( x >0)\\\\\n",
    "0 & ( x\\leqslant 0)\n",
    "\\end{cases}\\\\\n",
    "\\end{gather*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.2 Sigmoid 계층\n",
    "\n",
    "활성화 함수 Sigmoid의 수식:\n",
    "\n",
    "\\begin{gather*}\n",
    "y\\ =\\ \\frac{1}{1+e^{-x}}\n",
    "\\end{gather*}\n",
    "\n",
    "그의 미분:\n",
    "\n",
    "\\begin{gather*}\n",
    "y\\ =\\ \\frac{1}{1+e^{-x}}\\\\\n",
    "\\\\\n",
    "\\frac{\\partial L}{\\partial y} y^{2} e^{-x} =\\frac{\\partial L}{\\partial y} y( 1-y)\n",
    "\\end{gather*}\n",
    "\n",
    "구현:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out=None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=1/(1+np.exp(-x))\n",
    "        self.out=out\n",
    "        \n",
    "    def backward(self,dout):\n",
    "        dx=dout*(1.0-self.out)*self.out\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Affine/Softmax 계층 구현하기\n",
    "\n",
    "### 5.6.1 Affine 계층\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2, 3)\n",
      "(3,)\n",
      "[0.34060857 1.16929798 1.66637798]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.random.rand(2)\n",
    "W = np.random.rand(2,3)\n",
    "B = np.random.rand(3)\n",
    "\n",
    "print(X.shape)\n",
    "print(W.shape)\n",
    "print(B.shape)\n",
    "Y = np.dot(X,W)+B\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 순전파시에 위처럼 행렬곱으로 계산했다.\n",
    "역전파시에는 아래와 같은 식이 도출된다.\n",
    "\n",
    "\\begin{gather*}\n",
    "\\frac{\\partial L}{\\partial X} =\\frac{\\partial L}{\\partial Y} \\cdotp W^{T}\\\\\n",
    "\\\\\n",
    "\\frac{\\partial L}{\\partial W} =X^{T} \\cdotp \\frac{\\partial L}{\\partial Y}\n",
    "\\end{gather*}\n",
    "\n",
    "*(잘 이해안됨..)*\n",
    "\n",
    "### 5.6.2 배치용 Affine 계층\n",
    "\n",
    "X 하나에 대한 것이 아닌, X묶음인 배치에 대한 계산\n",
    "\n",
    "아래는 Affine 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self,W,b):\n",
    "        self.W=W\n",
    "        self.b=b\n",
    "        self.x=None\n",
    "        self.dW=None\n",
    "        self.db=None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        self.x=x\n",
    "        out=np.dot(x,self.W)+self.b\n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dx=np.dot(dout,self.W.T)\n",
    "        self.dW=np.dot(self.x.T,dout)\n",
    "        self.db=np.sum(dout,axis=0)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.3 Softmax-with-Loss 계층\n",
    "\n",
    "p.176\n",
    "신경망은 추론 기능, 학습 기능이 있는데, 추론할 때는 Softmax가 필요하지 않지만, 학습할때는 Softmax계층을 사용한다\n",
    "\n",
    "Softmax-with-Loss계층은 Softmax+Cross Entropy Error 계층을 붙인것\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss=None\n",
    "        self.y=None\n",
    "        self.t=None\n",
    "        \n",
    "    def forward(self,x,t):\n",
    "        self.t=t\n",
    "        self.y=softmax(x)\n",
    "        self.loss=cross_entropy_error(self.y,self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self,dout=1):\n",
    "        batch_size=self.t.shape[0]\n",
    "        dx=(self.y-self.t)/batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 오차역전파법 구현하기\n",
    "\n",
    "### 5.7.1 신경망 학습의 전체 그림\n",
    "\n",
    "0. 전제: 신경망의 학습이란, 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정\n",
    "1. 미니배치: 훈련 데이터 중 일부를 무작위로 가져온다(미니배치)\n",
    "2. 기울기 산출: 미니배치의 손실함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다.\n",
    "3. 매개변수 갱신: 가중치 매개변수를 기울기 방향으로 아주 조금 갱신\n",
    "4. 반복\n",
    "\n",
    "사실 수치 미분으로도 기울기를 계산할 수 있으나 느리다. 그래서 오차역전파법을 사용하는 것.\n",
    "\n",
    "### 5.7.2 오차역전파법을 적용한 신경망 구현하기\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
